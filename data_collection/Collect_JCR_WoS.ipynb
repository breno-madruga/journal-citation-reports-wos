{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the records of InCites Journal Citation Reports (Web of Science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T19:50:21.793498Z",
     "start_time": "2023-11-14T19:50:21.393583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import re, traceback, csv, pandas as pd, time, os\n",
    "import playwright._impl._errors as errors\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from twisted.internet.error import TCPTimedOutError, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the class of Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InCitesSpider:\n",
    "    def __init__(self, url, login, password):\n",
    "        self.__url_base = url\n",
    "        self.__username = login\n",
    "        self.__password = password\n",
    "        self.__max_attempts = 1\n",
    "        self.__user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                             \"Chrome/122.0.0.0 Safari/537.36 OPR/108.0.0.0\")\n",
    "        self.__data = None\n",
    "        self.__playwright = None\n",
    "        self.__browser = None\n",
    "        self.__page = None\n",
    "        os.environ[\"USER_AUTHENTICATED\"] = \"False\"\n",
    "\n",
    "    @property\n",
    "    def get_data(self):\n",
    "        return self.__data\n",
    "\n",
    "    async def __get_html(self, url, css_selector=None, to_close=True):\n",
    "        if self.__playwright is None:\n",
    "            self.__playwright = await async_playwright().start()\n",
    "        if self.__browser is None or not self.__browser.is_connected():\n",
    "            self.__browser = await self.__playwright.chromium.launch(headless=True, args=[\"--start-maximized\"])\n",
    "        if self.__page is None or self.__page.is_closed():\n",
    "            self.__page = await self.__browser.new_page(user_agent=self.__user_agent)\n",
    "            # self.__page = await self.__browser.new_page()\n",
    "        await self.__page.goto(url)\n",
    "        if css_selector is not None:\n",
    "            await self.__page.wait_for_selector(css_selector)\n",
    "        html = await self.__page.content()\n",
    "        if to_close:\n",
    "            await self.__browser.close()\n",
    "            await self.__playwright.stop()\n",
    "        return html\n",
    "\n",
    "    async def authenticate(self, num_attempt=0):\n",
    "        try:\n",
    "            # Initializing the webdriver.\n",
    "            await self.__get_html(self.__url_base, to_close=False)\n",
    "\n",
    "            # Clicking in the button \"Sign In\".\n",
    "            css_select = \"button[aria-label='Sign In']\"\n",
    "            button = self.__page.locator(css_select)\n",
    "            if await button.is_visible() and await button.is_enabled():\n",
    "                await button.click()\n",
    "\n",
    "            # Authenticating with user's account data.\n",
    "            username_field = self.__page.locator(\"input#mat-input-0\")\n",
    "            await username_field.wait_for(state=\"visible\")\n",
    "            await username_field.fill(self.__username)\n",
    "            password_field = self.__page.locator(\"input#mat-input-1\")\n",
    "            await password_field.wait_for(state=\"visible\")\n",
    "            await password_field.fill(self.__password)\n",
    "            await password_field.press(\"Enter\")\n",
    "\n",
    "            # Enabling the cookies.\n",
    "            css_select = \"button#onetrust-accept-btn-handler\"\n",
    "            button = self.__page.locator(css_select)\n",
    "            await button.wait_for(state=\"visible\", timeout=120000)\n",
    "            if await button.is_visible() and await button.is_enabled():\n",
    "                await button.click()\n",
    "\n",
    "            # Redirecting the list of journals.\n",
    "            css_select = \"div[aria-label='Journals']\"\n",
    "            button = self.__page.locator(css_select)\n",
    "            await button.scroll_into_view_if_needed()\n",
    "            await button.wait_for(state=\"visible\", timeout=120000)\n",
    "            if await button.is_visible() and await button.is_enabled():\n",
    "                await button.click()\n",
    "\n",
    "            os.environ[\"USER_AUTHENTICATED\"] = \"True\"\n",
    "        except (errors.TimeoutError, errors.TargetClosedError, errors.Error, AttributeError, Exception,\n",
    "                TCPTimedOutError, TimeoutError) as e:\n",
    "            print(f\"[ERROR-DEBUG] {e}: {self.__url_base}\")\n",
    "            print(\"\".join(traceback.format_tb(e.__traceback__)))\n",
    "            if num_attempt <= self.__max_attempts:\n",
    "                num_attempt += 1\n",
    "                print(f\"Number of attempting in 'authenticate': {num_attempt}\")\n",
    "                await self.authenticate(num_attempt)\n",
    "\n",
    "    async def parse_items(self, num_attempt=0):\n",
    "        self.__data = []\n",
    "        try:\n",
    "            num_item_per_page = None\n",
    "            flag = True\n",
    "\n",
    "            # Getting the number of journals.\n",
    "            element = self.__page.locator(\"div#liveRegion > p\")\n",
    "            await element.wait_for(state=\"visible\")\n",
    "            num_journals = await element.text_content()\n",
    "            num_journals = int(num_journals.replace(\",\", \"\").split(\" \")[0])\n",
    "\n",
    "            # Setting the number of items per page.\n",
    "            element = self.__page.locator(\"mat-select[aria-label='Items per page:']\")\n",
    "            await element.wait_for(state=\"visible\")\n",
    "            await element.scroll_into_view_if_needed()\n",
    "            await element.click()\n",
    "            element = self.__page.locator(\"div[aria-label='Items per page:'] > mat-option:nth-child(5)\")\n",
    "            await element.click()\n",
    "            css_select = \"mat-select[aria-label='Items per page:'] > div > div > span > span\"\n",
    "            element = self.__page.locator(css_select)\n",
    "            num_item_per_page = int(await element.text_content())\n",
    "            print(\"Number of Items per Page:\", num_item_per_page)\n",
    "\n",
    "            while flag:\n",
    "                # Showing the progress.\n",
    "                print(f\"Collected: {len(self.__data)} of {num_journals} ({((len(self.__data) / num_journals) * 100):.2f}%)\")\n",
    "                time.sleep(10)\n",
    "\n",
    "                # Waiting to load the records.\n",
    "                element = self.__page.locator(\"div.backdrop\")\n",
    "                await element.wait_for(state=\"hidden\", timeout=120000)\n",
    "\n",
    "                # Waiting to load the table of records.\n",
    "                table = self.__page.locator(\"section.table-section > mat-table[class*='mat-table']\")\n",
    "                await table.wait_for(state=\"visible\", timeout=120000)\n",
    "                if await table.is_visible():\n",
    "                    # Defining the scraper.\n",
    "                    html_soup = BeautifulSoup(await self.__page.content(), \"html.parser\")\n",
    "\n",
    "                    # Getting the rows.\n",
    "                    rows = html_soup.find_all(\"mat-row\")\n",
    "                    for idx, row in enumerate(rows):\n",
    "                        try:\n",
    "                            record = {}\n",
    "                            # Getting the columns/cells of data.\n",
    "                            cells = row.select(\"mat-cell > span\")\n",
    "\n",
    "                            # Journal name.\n",
    "                            record[\"journal_name\"] = re.sub(r\"\\s+\", \" \", cells[0].string).strip()\n",
    "\n",
    "                            # ISSN.\n",
    "                            record[\"issn\"] = re.sub(r\"\\s+\", \" \", cells[1].string).strip()\n",
    "\n",
    "                            # eISSN.\n",
    "                            record[\"e_issn\"] = re.sub(r\"\\s+\", \" \", cells[2].string).strip()\n",
    "\n",
    "                            # Category and Edition.\n",
    "                            if cells[3].select(\"span.multiple > mat-expansion-panel > div > div > div > span\"):\n",
    "                                items = cells[3].select(\"span.multiple > mat-expansion-panel > div > div > div > span\")\n",
    "                                record[\"category\"] = tuple([re.sub(r\"\\s+\", \" \", item.string).strip() for item in items])\n",
    "                                items = cells[4].select(\"span.table-cell-edition > mat-expansion-panel > div > div > span\")\n",
    "                                record[\"edition\"] = tuple([re.sub(r\"\\s+\", \" \", item.string).strip() for item in items])\n",
    "                            else:\n",
    "                                record[\"category\"] = re.sub(r\"\\s+\", \" \", cells[3].string).strip()\n",
    "                                record[\"edition\"] = tuple([re.sub(r\"\\s+\", \" \", item).strip() for item in cells[4].string.split(\",\")])\n",
    "\n",
    "                            # Total citations.\n",
    "                            record[\"total_citations\"] = re.sub(r\"\\s+\", \" \", cells[5].string).strip()\n",
    "\n",
    "                            # 2023 JIF.\n",
    "                            record[\"impact_factor_2023\"] = re.sub(r\"\\s+\", \" \", cells[6].string).strip()\n",
    "\n",
    "                            # JIF Quartile.\n",
    "                            if cells[7].select(\"span.multiple > mat-expansion-panel > div > div > span\"):\n",
    "                                items = cells[7].select(\"span.multiple > mat-expansion-panel > div > div > span\")\n",
    "                                record[\"jif_quartile\"] = tuple([re.sub(r\"\\s+\", \" \", item.string).strip() for item in items])\n",
    "                            else:\n",
    "                                record[\"jif_quartile\"] = re.sub(r\"\\s+\", \" \", cells[7].string).strip()\n",
    "\n",
    "                            # 2023 JCI.\n",
    "                            record[\"jci_2023\"] = re.sub(r\"\\s+\", \" \", cells[8].string).strip()\n",
    "\n",
    "                            # % of OA Gold.\n",
    "                            record[\"percent_oa_gold\"] = re.sub(r\"\\s+\", \" \", cells[9].string).strip()\n",
    "\n",
    "                            self.__data.append(record)\n",
    "                        except Exception as e:\n",
    "                            print(idx)\n",
    "                            raise e\n",
    "\n",
    "                    # Clicking the button.\n",
    "                    button = self.__page.locator(\"button.mat-paginator-navigation-next\")\n",
    "                    await button.scroll_into_view_if_needed()\n",
    "                    flag = await button.is_enabled()\n",
    "                    if flag:\n",
    "                        await button.click()\n",
    "                else:\n",
    "                    flag = False\n",
    "\n",
    "            # Closing the webdriver.\n",
    "            await self.__browser.close()\n",
    "            await self.__playwright.stop()\n",
    "        except (errors.TimeoutError, errors.TargetClosedError, errors.Error, AttributeError, Exception,\n",
    "                TCPTimedOutError, TimeoutError) as e:\n",
    "            print(f\"[ERROR-DEBUG] {e}: {self.__url_base}\")\n",
    "            print(\"\".join(traceback.format_tb(e.__traceback__)))\n",
    "            if num_attempt <= self.__max_attempts:\n",
    "                num_attempt += 1\n",
    "                print(f\"Number of attempting in 'parse_items': {num_attempt}\")\n",
    "                await self.parse_items(num_attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data from its URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T19:50:21.807434Z",
     "start_time": "2023-11-14T19:50:21.806121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the credentials.\n",
    "username = \">>> VALID USER/E-MAIL <<<\"\n",
    "password = \">>> YOUR PASSWORD <<<\"\n",
    "\n",
    "# Determining the URL of target page.\n",
    "url = \"https://jcr.clarivate.com/jcr/home\"\n",
    "\n",
    "# Creating the spider.\n",
    "spider = InCitesSpider(url, username, password)\n",
    "\n",
    "# Authenticating the valid user.\n",
    "await spider.authenticate()\n",
    "is_authenticated = bool(os.environ[\"USER_AUTHENTICATED\"])\n",
    "\n",
    "# Crawling the data.\n",
    "if is_authenticated:\n",
    "    await spider.parse_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the collected data.\n",
    "data = spider.get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:59:36.644644Z",
     "start_time": "2023-11-13T15:59:36.641852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the number of records collected.\n",
    "print(\"Number of records collected: {}.\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving the data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:59:36.716113Z",
     "start_time": "2023-11-13T15:59:36.644364Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the Pandas' DataFrame object.\n",
    "df_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:59:37.848999Z",
     "start_time": "2023-11-13T15:59:36.695980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data.\n",
    "df_data.replace({\"n/a\": None, \"N/A\": None}, inplace=True)\n",
    "df_data[[\"category\", \"edition\", \"jif_quartile\"]] = df_data[[\"category\", \"edition\", \"jif_quartile\"]].apply(\n",
    "    lambda x: x.apply(lambda y: tuple(y) if type(y) == list else y), axis=1)\n",
    "df_data.drop_duplicates(keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:59:37.870099Z",
     "start_time": "2023-11-13T15:59:37.862604Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking the information about the dataset.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:00:04.519009Z",
     "start_time": "2023-11-13T16:00:04.459942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exporting the data to CSV file.\n",
    "df_data.to_csv(\"jcr_2023_wos.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e313781e5f9487634688338feaea31cdeb3d0e175705036a80436c634137acb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
